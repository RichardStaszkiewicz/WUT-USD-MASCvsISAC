{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na0JFB-E4hUg",
        "outputId": "55eef37f-d859-43d2-f298-8052737c6bb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'BenchMARL' already exists and is not an empty directory.\n",
            "/content/BenchMARL\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Obtaining file:///content/BenchMARL\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torchrl~=0.6.0 in /usr/local/lib/python3.10/dist-packages (from benchmarl==1.3.0) (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from benchmarl==1.3.0) (4.67.1)\n",
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.10/dist-packages (from benchmarl==1.3.0) (1.3.2)\n",
            "Requirement already satisfied: torch>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchrl~=0.6.0->benchmarl==1.3.0) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl~=0.6.0->benchmarl==1.3.0) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl~=0.6.0->benchmarl==1.3.0) (24.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl~=0.6.0->benchmarl==1.3.0) (3.1.0)\n",
            "Requirement already satisfied: tensordict>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torchrl~=0.6.0->benchmarl==1.3.0) (0.6.2)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core->benchmarl==1.3.0) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core->benchmarl==1.3.0) (4.9.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.4,>=2.2->hydra-core->benchmarl==1.3.0) (6.0.2)\n",
            "Requirement already satisfied: orjson in /usr/local/lib/python3.10/dist-packages (from tensordict>=0.6.0->torchrl~=0.6.0->benchmarl==1.3.0) (3.10.12)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.5.0->torchrl~=0.6.0->benchmarl==1.3.0) (3.0.2)\n",
            "Building wheels for collected packages: benchmarl\n",
            "  Building editable for benchmarl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for benchmarl: filename=benchmarl-1.3.0-0.editable-py3-none-any.whl size=3799 sha256=90b7e6328f717916eb76632e0a2af3827ce7338baf31a345309bcc166a869fc5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k1jokk0k/wheels/5b/86/4d/eff20c27275b75bdf6b20e4a81af849509b0068ff4c44be9df\n",
            "Successfully built benchmarl\n",
            "Installing collected packages: benchmarl\n",
            "  Attempting uninstall: benchmarl\n",
            "    Found existing installation: benchmarl 1.3.0\n",
            "    Uninstalling benchmarl-1.3.0:\n",
            "      Successfully uninstalled benchmarl-1.3.0\n",
            "Successfully installed benchmarl-1.3.0\n",
            "Requirement already satisfied: dm-meltingpot in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (1.4.0)\n",
            "Collecting chex<0.1.81 (from dm-meltingpot)\n",
            "  Using cached chex-0.1.7-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: dm-env in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (1.6)\n",
            "Requirement already satisfied: dmlab2d in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (1.0.0)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (4.2.1)\n",
            "Requirement already satisfied: ml-collections in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (1.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (3.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (1.26.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (4.10.0.84)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (1.4.4)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (2.6.1)\n",
            "Requirement already satisfied: reactivex in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (4.0.4)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (from dm-meltingpot) (2.17.1)\n",
            "Requirement already satisfied: jax>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from chex<0.1.81->dm-meltingpot) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from chex<0.1.81->dm-meltingpot) (0.4.33)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex<0.1.81->dm-meltingpot) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex<0.1.81->dm-meltingpot) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml-collections->dm-meltingpot) (1.17.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml-collections->dm-meltingpot) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dm-meltingpot) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dm-meltingpot) (2024.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (75.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (1.17.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (1.68.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (2.17.1)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (3.5.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->dm-meltingpot) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->dm-meltingpot) (0.45.1)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.6->chex<0.1.81->dm-meltingpot) (1.13.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->dm-meltingpot) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->dm-meltingpot) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow->dm-meltingpot) (0.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->dm-meltingpot) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->dm-meltingpot) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->dm-meltingpot) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow->dm-meltingpot) (2024.12.14)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->dm-meltingpot) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->dm-meltingpot) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow->dm-meltingpot) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow->dm-meltingpot) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->dm-meltingpot) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow->dm-meltingpot) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow->dm-meltingpot) (0.1.2)\n",
            "Using cached chex-0.1.7-py3-none-any.whl (89 kB)\n",
            "Installing collected packages: chex\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.88\n",
            "    Uninstalling chex-0.1.88:\n",
            "      Successfully uninstalled chex-0.1.88\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "optax 0.2.4 requires chex>=0.1.87, but you have chex 0.1.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chex-0.1.7\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Fetched 261 kB in 1s (294 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "x11-utils is already the newest version (7.7+5build2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.12).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "imagemagick is already the newest version (8:6.9.11.60+dfsg-1.3ubuntu0.22.04.5).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n",
            "Requirement already satisfied: id-marl-eval in /usr/local/lib/python3.10/dist-packages (0.0.4)\n",
            "Requirement already satisfied: colorcet==3.0.0 in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (3.0.0)\n",
            "Requirement already satisfied: matplotlib>=3.5.3 in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (3.8.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (1.26.4)\n",
            "Requirement already satisfied: rliable in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (1.2.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (0.13.2)\n",
            "Requirement already satisfied: pandas==1.4.4 in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (1.4.4)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (3.1.4)\n",
            "Requirement already satisfied: importlib-metadata<5.0 in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (4.13.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (0.4.6)\n",
            "Requirement already satisfied: neptune in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (1.13.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from id-marl-eval) (4.67.1)\n",
            "Requirement already satisfied: param>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from colorcet==3.0.0->id-marl-eval) (2.2.0)\n",
            "Requirement already satisfied: pyct>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from colorcet==3.0.0->id-marl-eval) (0.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.4->id-marl-eval) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.4->id-marl-eval) (2024.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<5.0->id-marl-eval) (3.21.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.3->id-marl-eval) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->id-marl-eval) (3.0.2)\n",
            "Requirement already satisfied: GitPython>=2.0.8 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (3.1.43)\n",
            "Requirement already satisfied: PyJWT in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (2.10.1)\n",
            "Requirement already satisfied: boto3>=1.28.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (1.36.2)\n",
            "Requirement already satisfied: bravado<12.0.0,>=11.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (11.0.3)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (1.0.0)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (3.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (1.17.0)\n",
            "Requirement already satisfied: swagger-spec-validator>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (4.12.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (2.2.3)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune->id-marl-eval) (1.8.0)\n",
            "Requirement already satisfied: arch<8.0,>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from rliable->id-marl-eval) (7.2.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from rliable->id-marl-eval) (1.13.1)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from rliable->id-marl-eval) (1.4.0)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch<8.0,>=5.3.1->rliable->id-marl-eval) (0.14.4)\n",
            "Requirement already satisfied: botocore<1.37.0,>=1.36.2 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune->id-marl-eval) (1.36.2)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune->id-marl-eval) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from boto3>=1.28.0->neptune->id-marl-eval) (0.11.1)\n",
            "Requirement already satisfied: bravado-core>=5.16.1 in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (6.1.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (1.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (6.0.2)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (3.19.3)\n",
            "Requirement already satisfied: monotonic in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (1.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython>=2.0.8->neptune->id-marl-eval) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune->id-marl-eval) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune->id-marl-eval) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune->id-marl-eval) (2024.12.14)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune->id-marl-eval) (4.23.0)\n",
            "Requirement already satisfied: importlib-resources>=1.3 in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune->id-marl-eval) (6.4.5)\n",
            "Requirement already satisfied: jsonref in /usr/local/lib/python3.10/dist-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (1.1.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune->id-marl-eval) (5.0.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune->id-marl-eval) (24.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune->id-marl-eval) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune->id-marl-eval) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune->id-marl-eval) (0.22.3)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch<8.0,>=5.3.1->rliable->id-marl-eval) (1.0.1)\n",
            "Requirement already satisfied: fqdn in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (3.0.0)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>0.1.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (0.1.1)\n",
            "Requirement already satisfied: uri-template in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (1.3.0)\n",
            "Requirement already satisfied: webcolors>=24.6.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (24.11.1)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (1.3.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.10/dist-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune->id-marl-eval) (2.9.0.20241206)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "python3-opengl is already the newest version (3.1.5+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 81 not upgraded.\n",
            "Requirement already satisfied: chex in /usr/local/lib/python3.10/dist-packages (0.1.7)\n",
            "Collecting chex\n",
            "  Using cached chex-0.1.88-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex) (1.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex) (4.12.2)\n",
            "Requirement already satisfied: jax>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from chex) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.4.27 in /usr/local/lib/python3.10/dist-packages (from chex) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from chex) (1.26.4)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->chex) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->chex) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.27->chex) (1.13.1)\n",
            "Using cached chex-0.1.88-py3-none-any.whl (99 kB)\n",
            "Installing collected packages: chex\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.7\n",
            "    Uninstalling chex-0.1.7:\n",
            "      Successfully uninstalled chex-0.1.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dm-meltingpot 2.2.0 requires chex<0.1.81, but you have chex 0.1.88 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed chex-0.1.88\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.10.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
            "Requirement already satisfied: vmas[all] in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "\u001b[33mWARNING: vmas 1.4.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vmas[all]) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vmas[all]) (2.5.1+cu121)\n",
            "Requirement already satisfied: pyglet<=1.5.27 in /usr/local/lib/python3.10/dist-packages (from vmas[all]) (1.5.27)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from vmas[all]) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from vmas[all]) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->vmas[all]) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->vmas[all]) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vmas[all]) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->vmas[all]) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vmas[all]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vmas[all]) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vmas[all]) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->vmas[all]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->vmas[all]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vmas[all]) (3.0.2)\n",
            "Requirement already satisfied: vmas in /usr/local/lib/python3.10/dist-packages (1.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from vmas) (1.26.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from vmas) (2.5.1+cu121)\n",
            "Requirement already satisfied: pyglet<=1.5.27 in /usr/local/lib/python3.10/dist-packages (from vmas) (1.5.27)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from vmas) (0.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from vmas) (1.17.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->vmas) (3.1.0)\n",
            "Requirement already satisfied: gym_notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->vmas) (0.0.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->vmas) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->vmas) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->vmas) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->vmas) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->vmas) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->vmas) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->vmas) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->vmas) (3.0.2)\n",
            "1.4.3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7224f7591630>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#@title\n",
        "!git clone https://github.com/facebookresearch/BenchMARL\n",
        "#@title\n",
        "%cd /content/BenchMARL\n",
        "#@title\n",
        "!pip install -U torch torchvision\n",
        "!pip install -e .\n",
        "#@title\n",
        "!pip install dm-meltingpot\n",
        "!apt-get update\n",
        "!apt-get install -y x11-utils\n",
        "!apt-get install -y xvfb\n",
        "!apt-get install -y imagemagick\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install id-marl-eval\n",
        "!apt-get install python3-opengl\n",
        "!pip install --upgrade chex\n",
        "!pip install wandb\n",
        "!pip install vmas[all]\n",
        "!python -m pip install vmas\n",
        "!python -c \"import vmas; print(vmas.__version__)\"\n",
        "import pyvirtualdisplay\n",
        "display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nX5hqqJXfw_c",
        "outputId": "6676c2f2-44e4-4acb-e594-def2f95e832e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m01019799\u001b[0m (\u001b[33m01019799-marcingodniak\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "from benchmarl.eval_results import load_and_merge_json_dicts, Plotting\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkqvAnrLdCPd",
        "outputId": "f0a53db2-bd59-4cd7-dc77-2e2075f08670"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/BenchMARL/benchmarl/environments/magent/common.py:45: DeprecationWarning: private variables, such as '_MAgentTask__get_env', will be normal attributes in 3.11\n",
            "  def __get_env(self, config) -> EnvBase:\n",
            "/usr/local/lib/python3.10/dist-packages/colorcet/__init__.py:82: MatplotlibDeprecationWarning: The register_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps.register(name)`` instead.\n",
            "  register_cmap(\"cet_\"+name, cmap=cm[name])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function configures and returns the experiment setup by using the `ExperimentConfig` class from the `benchmarl` package. The configuration includes parameters like logging settings, iteration limits, and computational resources. We decided to override some of the default experiment parameters.\n",
        "\n",
        "\n",
        "| **Argument**                        | **Type**           | **Description**                                                                                                                                                 |\n",
        "|-------------------------------------|--------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `experiment_config.save_folder`     | `Path`             | Directory where the experiment data (such as logs, checkpoints) will be saved. By default, it is set to the current working directory (`Path.cwd()`).          |\n",
        "| `experiment_config.loggers`         | `list of str`      | List of logging tools to be used during the experiment. In this case, the only logger specified is `\"wandb\"`, which stands for Weights & Biases.                |\n",
        "| `experiment_config.max_n_iters`     | `int`              | Maximum number of iterations for the experiment. In this function, it is set to 200.                                                                          |\n",
        "| `experiment_config.eval_frequency`  | `int`              | Defines the frequency (in terms of iterations) at which evaluation is done. Set to 5 iterations in this case.                                                  |\n",
        "| `experiment_config.checkpoint_frequency` | `int`         | Defines how often (in terms of iterations) a checkpoint will be saved. Set to 2 iterations here.                                                              |\n",
        "| `experiment_config.verbose`         | `bool`             | Whether to print detailed logs during the experiment. If `True`, it enables verbose output.                                                                   |\n",
        "| `experiment_config.num_workers`     | `int`              | The number of workers (or threads) used for data loading and parallelization. Set to 4 in this case.                                                           |\n",
        "| `experiment_config.device`          | `str`              | Specifies the device used for computation. Options are typically `\"cuda\"` (GPU) or `\"cpu\"`. Here, it is set to `\"cuda\"`, meaning the experiment will run on a GPU. |"
      ],
      "metadata": {
        "id": "GrFI3hZ95KvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_experiment():\n",
        "    \"\"\"Configures and returns the experiment setup.\"\"\"\n",
        "    from benchmarl.experiment import ExperimentConfig\n",
        "    from pathlib import Path\n",
        "\n",
        "    # Create the experiment configuration\n",
        "    experiment_config = ExperimentConfig.get_from_yaml()\n",
        "\n",
        "    experiment_config.save_folder = Path.cwd()\n",
        "    experiment_config.loggers = [\"wandb\"]\n",
        "    experiment_config.max_n_iters = 200\n",
        "    experiment_config.eval_frequency = 5\n",
        "    experiment_config.checkpoint_frequency = 2\n",
        "    experiment_config.verbose = False\n",
        "    experiment_config.num_workers = 4\n",
        "    experiment_config.device = \"cuda\"\n",
        "\n",
        "    # Return the configured experiment\n",
        "    return experiment_config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Deo4I4_GcxK8",
        "outputId": "dcd6dbcc-31e4-4ad2-abcc-4447d5c771fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tasks Overview\n",
        "\n",
        "This function configures and returns a list of tasks for the experiment setup.  Below are the brief descriptions of each task:\n",
        "\n",
        "### Tasks:\n",
        "\n",
        "1. **Simple Spread**  \n",
        "   A cooperative task where agents must spread across landmarks while avoiding collisions.\n",
        "\n",
        "2. **Simple Adversary**  \n",
        "   A competitive task where agents try to reach a target landmark while avoiding an adversary.\n",
        "\n",
        "3. **Navigation**  \n",
        "   A light cooperative task where agents navigate to their goals while avoiding collisions.\n",
        "\n",
        "4. **Simple World Comm**  \n",
        "   A mixed task involving both cooperative and competitive elements, with agents avoiding adversaries and obstacles while interacting in a predator-prey setup.\n",
        "\n",
        "Each task is configured using settings from the respective YAML containg default parameters specyfic to given task.\n"
      ],
      "metadata": {
        "id": "_B49hW0Q6RCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_tasks():\n",
        "    \"\"\"Returns a list of configured tasks and prints their configurations.\"\"\"\n",
        "    from benchmarl.environments import VmasTask\n",
        "\n",
        "    task_map = {\n",
        "        'Simple Spread': VmasTask.SIMPLE_SPREAD,\n",
        "        'Simple Adversary': VmasTask.SIMPLE_ADVERSARY,\n",
        "        'Navigation': VmasTask.NAVIGATION,\n",
        "        'Simple World Comm': VmasTask.SIMPLE_WORLD_COMM,\n",
        "    }\n",
        "\n",
        "    tasks = []\n",
        "    for task_name, task_class in task_map.items():\n",
        "        task = task_class.get_from_yaml()\n",
        "        tasks.append((task_name, task))\n",
        "\n",
        "    print(\"Tasks: \")\n",
        "    for i, (task_name, task) in enumerate(tasks, start=1):\n",
        "        print(f\"Task {i}: {task_name}\")\n",
        "        print(f\"Configuration: {task.config}\")\n",
        "        print(\"-\" * 50)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    return [task for _, task in tasks]"
      ],
      "metadata": {
        "id": "oggH8Atcc1lP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithms Overview\n",
        "\n",
        "This function configures and returns a list of algorithms for the experiment setup. Below are brief descriptions of the two algorithms and the shared parameter values:\n",
        "\n",
        "### Algorithms:\n",
        "\n",
        "1. **ISAC (Independent Actor-Critic)**  \n",
        "   A reinforcement learning algorithm where each agent has its own actor and critic. The critics operate independently, leading to higher policy entropy and more exploration, though coordination among agents may be more difficult.\n",
        "\n",
        "2. **MASAC (Multi-Agent Soft Actor-Critic)**  \n",
        "   A centralized version of the actor-critic algorithm, where a global critic models the interactions between all agents. MASAC tends to perform faster convergence and better coordination between agents compared to ISAC.\n",
        "\n",
        "Both algorithms share the following configuration values:\n",
        "\n",
        "### Shared Parameter Values:\n",
        "\n",
        "| Parameter                          | Value                             | Description                                                                                   |\n",
        "|------------------------------------|-----------------------------------|-----------------------------------------------------------------------------------------------|\n",
        "| `share_param_critic`               | `True`                            | Whether the critic parameters are shared between agents.                                       |\n",
        "| `num_qvalue_nets`                  | `2`                               | The number of Q-value networks used in the critic.                                             |\n",
        "| `loss_function`                    | `'l2'`                            | The loss function used for training the Q-value networks (L2 loss in this case).               |\n",
        "| `delay_qvalue`                     | `True`                            | Whether the Q-value update is delayed to avoid overfitting.                                    |\n",
        "| `target_entropy`                   | `'auto'`                          | Target entropy used for adjusting exploration; `'auto'` allows automatic tuning.              |\n",
        "| `discrete_target_entropy_weight`   | `0.2`                             | Weight applied to discrete target entropy in the reward function.                             |\n",
        "| `alpha_init`                       | `1.0`                             | Initial value of the entropy regularization coefficient (`alpha`).                             |\n",
        "| `min_alpha`                        | `None`                            | Minimum value of `alpha` to avoid too much regularization.                                     |\n",
        "| `max_alpha`                        | `None`                            | Maximum value of `alpha` to avoid too little regularization.                                  |\n",
        "| `fixed_alpha`                      | `False`                           | Whether to fix the value of `alpha` during training.                                           |\n",
        "| `scale_mapping`                    | `'biased_softplus_1.0'`           | The function used to map actions to the desired scale during training (biased softplus mapping).|\n",
        "| `use_tanh_normal`                  | `True`                            | Whether to use Tanh-normal distribution for sampling actions.                                  |\n",
        "\n",
        "Both ISAC and MASAC are configured using settings from their respective YAML files and can be further customized for specific use cases.\n"
      ],
      "metadata": {
        "id": "MzlU4-Vz7xSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_algorithms():\n",
        "    \"\"\"Returns a list of configured algorithm configurations and prints their names and configurations.\"\"\"\n",
        "    from benchmarl.algorithms import IsacConfig, MasacConfig\n",
        "\n",
        "    # Define the algorithms and their names\n",
        "    algorithms = [\n",
        "        ('ISAC', IsacConfig.get_from_yaml()),\n",
        "        ('MASAC', MasacConfig.get_from_yaml()),\n",
        "    ]\n",
        "\n",
        "    # Print the name and configuration of each algorithm\n",
        "    print(\"Algorithms: \")\n",
        "    for i, (algo_name, algo_config) in enumerate(algorithms, start=1):\n",
        "        print(f\"Algorithm {i}: {algo_name}\")\n",
        "        print(f\"Configuration: {algo_config}\")  # Access configuration using .config\n",
        "        print(\"-\" * 50)\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "    return [algo_config for _, algo_config in algorithms]"
      ],
      "metadata": {
        "id": "-bSAWzRTc5t2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model and Critic Configuration\n",
        "\n",
        "In this setup, both the **actor** and the **critic** will use a **Multi-Layer Perceptron (MLP)** architecture. An MLP is a type of feedforward neural network that is commonly used for approximating functions in reinforcement learning tasks.\n",
        "\n",
        "- **Actor Model**: The actor uses the MLP to decide on actions based on the current state of the environment.\n",
        "- **Critic Model**: The critic also uses the MLP to estimate the value of the actions taken by the actor, providing feedback for improvement.\n",
        "\n",
        "| **Parameter**          | **Description**                                                                                                                                                           |\n",
        "|------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| `num_cells`            | A list specifying the number of neurons in each layer of the MLP. In this case, the MLP consists of two layers, each with 256 neurons.                                     |\n",
        "| `layer_class`          | Defines the type of layer used in the MLP. Here, it is a `Linear` layer, which is a fully connected layer.                                                               |\n",
        "| `activation_class`     | Specifies the activation function used between layers. In this case, `Tanh` is applied, which is a common activation function for neural networks.                        |"
      ],
      "metadata": {
        "id": "0CF3B8wg9G3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def configure_models():\n",
        "    \"\"\"Returns the model and critic model configurations.\"\"\"\n",
        "    from benchmarl.models.mlp import MlpConfig\n",
        "\n",
        "    model_config = MlpConfig.get_from_yaml()         # Actor model configuration\n",
        "    critic_model_config = MlpConfig.get_from_yaml()  # Critic model configuration\n",
        "\n",
        "    print(\"Model Configuration:\")\n",
        "    print(model_config)\n",
        "    print(\"Critic Model Configuration:\")\n",
        "    print(critic_model_config)\n",
        "\n",
        "    return model_config, critic_model_config\n"
      ],
      "metadata": {
        "id": "v5RgqaUmc_In"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_benchmark() -> List[str]:\n",
        "    \"\"\"Runs the benchmark and returns a list of experiment JSON file paths.\"\"\"\n",
        "    from benchmarl.benchmark import Benchmark\n",
        "\n",
        "    # Configure experiment, tasks, algorithms, and models\n",
        "    experiment_config = configure_experiment()\n",
        "    tasks = configure_tasks()\n",
        "    algorithm_configs = configure_algorithms()\n",
        "    model_config, critic_model_config = configure_models()\n",
        "\n",
        "    # Initialize benchmark\n",
        "    benchmark = Benchmark(\n",
        "        algorithm_configs=algorithm_configs,\n",
        "        tasks=tasks,\n",
        "        seeds={0},\n",
        "        experiment_config=experiment_config,\n",
        "        model_config=model_config,\n",
        "        critic_model_config=critic_model_config,\n",
        "    )\n",
        "\n",
        "    # Run experiments\n",
        "    experiments = benchmark.get_experiments()\n",
        "    experiments_json_files = []\n",
        "    for experiment in experiments:\n",
        "        exp_json_file = str(\n",
        "            Path(experiment.folder_name) / Path(experiment.name + \".json\")\n",
        "        )\n",
        "        experiments_json_files.append(exp_json_file)\n",
        "        experiment.run()\n",
        "    return experiments_json_files"
      ],
      "metadata": {
        "id": "KJ2Lx2uidwXz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_results(experiments_json_files: List[str]):\n",
        "    \"\"\"Processes experiment results and generates comparison matrices.\"\"\"\n",
        "    # Load and process experiment outputs\n",
        "    raw_dict = load_and_merge_json_dicts(experiments_json_files)\n",
        "    processed_data = Plotting.process_data(raw_dict)\n",
        "    (\n",
        "        environment_comparison_matrix,\n",
        "        sample_efficiency_matrix,\n",
        "    ) = Plotting.create_matrices(processed_data, env_name=\"vmas\")\n",
        "    return processed_data, environment_comparison_matrix, sample_efficiency_matrix"
      ],
      "metadata": {
        "id": "rCjWP3GadySQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_plots(processed_data, environment_comparison_matrix, sample_efficiency_matrix):\n",
        "    \"\"\"Generates and saves all required plots.\"\"\"\n",
        "    # Performance profile plot\n",
        "    Plotting.performance_profile_figure(\n",
        "        environment_comparison_matrix=environment_comparison_matrix\n",
        "    )\n",
        "    plt.savefig(\"performance.png\")\n",
        "\n",
        "    # Aggregate scores plot\n",
        "    Plotting.aggregate_scores(\n",
        "        environment_comparison_matrix=environment_comparison_matrix\n",
        "    )\n",
        "    plt.savefig(\"aggregate_scores.png\")\n",
        "\n",
        "    # Sample efficiency curves\n",
        "    Plotting.environemnt_sample_efficiency_curves(\n",
        "        sample_effeciency_matrix=sample_efficiency_matrix\n",
        "    )\n",
        "    plt.savefig(\"sample_efficiency.png\")\n",
        "\n",
        "    # Task-specific sample efficiency curves for Simple Spread and Simple Adversary\n",
        "    Plotting.task_sample_efficiency_curves(\n",
        "        processed_data=processed_data, env=\"vmas\", task=\"simple_spread\"\n",
        "    )\n",
        "    plt.savefig(\"task_sample_efficiency_spread.png\")\n",
        "\n",
        "    Plotting.task_sample_efficiency_curves(\n",
        "        processed_data=processed_data, env=\"vmas\", task=\"simple_adversary\"\n",
        "    )\n",
        "    plt.savefig(\"task_sample_efficiency_adversary.png\")\n",
        "\n",
        "    # Task-specific sample efficiency curves for Navigation and Simple World Comm\n",
        "    Plotting.task_sample_efficiency_curves(\n",
        "        processed_data=processed_data, env=\"vmas\", task=\"navigation\"\n",
        "    )\n",
        "    plt.savefig(\"task_sample_efficiency_navigation.png\")\n",
        "\n",
        "    Plotting.task_sample_efficiency_curves(\n",
        "        processed_data=processed_data, env=\"vmas\", task=\"simple_world_comm\"\n",
        "    )\n",
        "    plt.savefig(\"task_sample_efficiency_world_comm.png\")\n",
        "\n",
        "    # Display all plots\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "UBkH2oYdd0gK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gHFePaoK5th1",
        "outputId": "dceb2720-e21a-4471-b0e7-24035fc7d5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tasks: \n",
            "Task 1: Simple Spread\n",
            "Configuration: {'max_steps': 100, 'obs_agents': True, 'n_agents': 3}\n",
            "--------------------------------------------------\n",
            "Task 2: Simple Adversary\n",
            "Configuration: {'max_steps': 100, 'n_agents': 3, 'n_adversaries': 1}\n",
            "--------------------------------------------------\n",
            "Task 3: Navigation\n",
            "Configuration: {'max_steps': 100, 'n_agents': 3, 'collisions': True, 'agents_with_same_goal': 1, 'observe_all_goals': False, 'shared_rew': False, 'split_goals': False, 'lidar_range': 0.35, 'agent_radius': 0.1}\n",
            "--------------------------------------------------\n",
            "Task 4: Simple World Comm\n",
            "Configuration: {'max_steps': 100, 'num_good_agents': 2, 'num_adversaries': 4, 'num_landmarks': 1, 'num_food': 2, 'num_forests': 2}\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Algorithms: \n",
            "Algorithm 1: ISAC\n",
            "Configuration: IsacConfig(share_param_critic=True, num_qvalue_nets=2, loss_function='l2', delay_qvalue=True, target_entropy='auto', discrete_target_entropy_weight=0.2, alpha_init=1.0, min_alpha=None, max_alpha=None, fixed_alpha=False, scale_mapping='biased_softplus_1.0', use_tanh_normal=True)\n",
            "--------------------------------------------------\n",
            "Algorithm 2: MASAC\n",
            "Configuration: MasacConfig(share_param_critic=True, num_qvalue_nets=2, loss_function='l2', delay_qvalue=True, target_entropy='auto', discrete_target_entropy_weight=0.2, alpha_init=1.0, min_alpha=None, max_alpha=None, fixed_alpha=False, scale_mapping='biased_softplus_1.0', use_tanh_normal=True)\n",
            "--------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "Model Configuration:\n",
            "MlpConfig(num_cells=[256, 256], layer_class=<class 'torch.nn.modules.linear.Linear'>, activation_class=<class 'torch.nn.modules.activation.Tanh'>, activation_kwargs=None, norm_class=None, norm_kwargs=None)\n",
            "Critic Model Configuration:\n",
            "MlpConfig(num_cells=[256, 256], layer_class=<class 'torch.nn.modules.linear.Linear'>, activation_class=<class 'torch.nn.modules.activation.Tanh'>, activation_kwargs=None, norm_class=None, norm_kwargs=None)\n",
            "Created benchmark with 8 experiments.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39/wandb/run-20250119_191439-isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39' target=\"_blank\">isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please import `sobel` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n",
            "mean return = -524.8851928710938: 100%|| 10/10 [02:53<00:00, 17.14s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/agents/alpha</td><td></td></tr><tr><td>train/agents/entropy</td><td></td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agents/loss_actor</td><td></td></tr><tr><td>train/agents/loss_alpha</td><td></td></tr><tr><td>train/agents/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/reward/episode_reward_max</td><td>-237.67279</td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td>-524.88519</td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td>-984.39862</td></tr><tr><td>collection/reward/episode_reward_max</td><td>-237.67279</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>-524.88519</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-984.39862</td></tr><tr><td>collection/reward/reward_max</td><td>-1.77253</td></tr><tr><td>collection/reward/reward_mean</td><td>-5.24885</td></tr><tr><td>collection/reward/reward_min</td><td>-13.41236</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td>-332.00555</td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td>-525.80103</td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td>-843.07672</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>-332.00555</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>-525.80103</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-843.07672</td></tr><tr><td>timers/collection_time</td><td>2.06348</td></tr><tr><td>timers/evaluation_time</td><td>0.93709</td></tr><tr><td>timers/iteration_time</td><td>17.03601</td></tr><tr><td>timers/total_time</td><td>173.34123</td></tr><tr><td>timers/training_time</td><td>14.97185</td></tr><tr><td>train/agents/alpha</td><td>0.62194</td></tr><tr><td>train/agents/entropy</td><td>1.36389</td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td>0.29218</td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td>2.36389</td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td>16.44452</td></tr><tr><td>train/agents/loss_actor</td><td>47.21301</td></tr><tr><td>train/agents/loss_alpha</td><td>-1.12289</td></tr><tr><td>train/agents/loss_qvalue</td><td>42.83772</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39/wandb/run-20250119_191439-isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (isac_simple_spread_mlp__ecf6eff9_25_01_19-19_14_39) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = -524.8851928710938: 100%|| 10/10 [02:56<00:00, 17.62s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36/wandb/run-20250119_191736-isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36' target=\"_blank\">isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = -42.13432312011719: 100%|| 10/10 [04:55<00:00, 30.62s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/adversary/reward/episode_reward_max</td><td></td></tr><tr><td>collection/adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/adversary/reward/episode_reward_min</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_max</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_min</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/adversary/alpha</td><td></td></tr><tr><td>train/adversary/entropy</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_actor</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/adversary/loss_actor</td><td></td></tr><tr><td>train/adversary/loss_alpha</td><td></td></tr><tr><td>train/adversary/loss_qvalue</td><td></td></tr><tr><td>train/agent/alpha</td><td></td></tr><tr><td>train/agent/entropy</td><td></td></tr><tr><td>train/agent/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agent/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agent/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agent/loss_actor</td><td></td></tr><tr><td>train/agent/loss_alpha</td><td></td></tr><tr><td>train/agent/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/adversary/reward/episode_reward_max</td><td>-15.82831</td></tr><tr><td>collection/adversary/reward/episode_reward_mean</td><td>-102.32653</td></tr><tr><td>collection/adversary/reward/episode_reward_min</td><td>-208.10947</td></tr><tr><td>collection/agent/reward/episode_reward_max</td><td>191.55283</td></tr><tr><td>collection/agent/reward/episode_reward_mean</td><td>18.05789</td></tr><tr><td>collection/agent/reward/episode_reward_min</td><td>-114.79193</td></tr><tr><td>collection/reward/episode_reward_max</td><td>-8.27832</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>-42.13432</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-110.53436</td></tr><tr><td>collection/reward/reward_max</td><td>2.0451</td></tr><tr><td>collection/reward/reward_mean</td><td>0.18058</td></tr><tr><td>collection/reward/reward_min</td><td>-1.34408</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/adversary/reward/episode_reward_max</td><td>-37.8466</td></tr><tr><td>eval/adversary/reward/episode_reward_mean</td><td>-127.16484</td></tr><tr><td>eval/adversary/reward/episode_reward_min</td><td>-220.4198</td></tr><tr><td>eval/agent/reward/episode_reward_max</td><td>95.79637</td></tr><tr><td>eval/agent/reward/episode_reward_mean</td><td>21.6144</td></tr><tr><td>eval/agent/reward/episode_reward_min</td><td>-116.62258</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>-11.54662</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>-52.77523</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-77.23459</td></tr><tr><td>timers/collection_time</td><td>2.4678</td></tr><tr><td>timers/evaluation_time</td><td>0.82257</td></tr><tr><td>timers/iteration_time</td><td>29.71045</td></tr><tr><td>timers/total_time</td><td>295.30429</td></tr><tr><td>timers/training_time</td><td>27.24175</td></tr><tr><td>train/adversary/alpha</td><td>0.62197</td></tr><tr><td>train/adversary/entropy</td><td>1.3606</td></tr><tr><td>train/adversary/grad_norm_loss_actor</td><td>0.54132</td></tr><tr><td>train/adversary/grad_norm_loss_alpha</td><td>2.3606</td></tr><tr><td>train/adversary/grad_norm_loss_qvalue</td><td>2.23434</td></tr><tr><td>train/adversary/loss_actor</td><td>1.32613</td></tr><tr><td>train/adversary/loss_alpha</td><td>-1.12119</td></tr><tr><td>train/adversary/loss_qvalue</td><td>0.40311</td></tr><tr><td>train/agent/alpha</td><td>0.62192</td></tr><tr><td>train/agent/entropy</td><td>1.36378</td></tr><tr><td>train/agent/grad_norm_loss_actor</td><td>0.38572</td></tr><tr><td>train/agent/grad_norm_loss_alpha</td><td>2.36378</td></tr><tr><td>train/agent/grad_norm_loss_qvalue</td><td>3.52483</td></tr><tr><td>train/agent/loss_actor</td><td>-10.2285</td></tr><tr><td>train/agent/loss_alpha</td><td>-1.12291</td></tr><tr><td>train/agent/loss_qvalue</td><td>2.24116</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36/wandb/run-20250119_191736-isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (isac_simple_adversary_mlp__64662c9c_25_01_19-19_17_36) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = -42.13432312011719: 100%|| 10/10 [04:57<00:00, 29.78s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/isac_navigation_mlp__3ede7f36_25_01_19-19_22_35/wandb/run-20250119_192235-isac_navigation_mlp__3ede7f36_25_01_19-19_22_35</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_navigation_mlp__3ede7f36_25_01_19-19_22_35' target=\"_blank\">isac_navigation_mlp__3ede7f36_25_01_19-19_22_35</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_navigation_mlp__3ede7f36_25_01_19-19_22_35' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_navigation_mlp__3ede7f36_25_01_19-19_22_35</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = -0.26714327931404114: 100%|| 10/10 [03:26<00:00, 19.94s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/info/agent_collisions</td><td></td></tr><tr><td>collection/agents/info/final_rew</td><td></td></tr><tr><td>collection/agents/info/pos_rew</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/agents/alpha</td><td></td></tr><tr><td>train/agents/entropy</td><td></td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agents/loss_actor</td><td></td></tr><tr><td>train/agents/loss_alpha</td><td></td></tr><tr><td>train/agents/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/info/agent_collisions</td><td>-0.00244</td></tr><tr><td>collection/agents/info/final_rew</td><td>0</td></tr><tr><td>collection/agents/info/pos_rew</td><td>-0.00023</td></tr><tr><td>collection/agents/reward/episode_reward_max</td><td>0.22987</td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td>-0.26714</td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td>-4.17688</td></tr><tr><td>collection/reward/episode_reward_max</td><td>0.22987</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>-0.26714</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-4.17688</td></tr><tr><td>collection/reward/reward_max</td><td>0.02582</td></tr><tr><td>collection/reward/reward_mean</td><td>-0.00267</td></tr><tr><td>collection/reward/reward_min</td><td>-1.01596</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td>0.1181</td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td>0.02603</td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td>-0.05419</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>0.1181</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>0.02603</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-0.05419</td></tr><tr><td>timers/collection_time</td><td>3.11839</td></tr><tr><td>timers/evaluation_time</td><td>2.24251</td></tr><tr><td>timers/iteration_time</td><td>19.30293</td></tr><tr><td>timers/total_time</td><td>206.5031</td></tr><tr><td>timers/training_time</td><td>16.18383</td></tr><tr><td>train/agents/alpha</td><td>0.62196</td></tr><tr><td>train/agents/entropy</td><td>1.36496</td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td>0.19324</td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td>2.36496</td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td>1.58818</td></tr><tr><td>train/agents/loss_actor</td><td>-8.29804</td></tr><tr><td>train/agents/loss_alpha</td><td>-1.12331</td></tr><tr><td>train/agents/loss_qvalue</td><td>1.155</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">isac_navigation_mlp__3ede7f36_25_01_19-19_22_35</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_navigation_mlp__3ede7f36_25_01_19-19_22_35' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_navigation_mlp__3ede7f36_25_01_19-19_22_35</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./isac_navigation_mlp__3ede7f36_25_01_19-19_22_35/wandb/run-20250119_192235-isac_navigation_mlp__3ede7f36_25_01_19-19_22_35/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (isac_navigation_mlp__3ede7f36_25_01_19-19_22_35) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = -0.26714327931404114: 100%|| 10/10 [03:29<00:00, 20.94s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05/wandb/run-20250119_192605-isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05' target=\"_blank\">isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = 7.95801305770874: 100%|| 10/10 [09:57<00:00, 59.32s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/adversary/reward/episode_reward_max</td><td></td></tr><tr><td>collection/adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/adversary/reward/episode_reward_min</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_min</td><td></td></tr><tr><td>collection/lead_adversary/reward/episode_reward_max</td><td></td></tr><tr><td>collection/lead_adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/lead_adversary/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_max</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_min</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_min</td><td></td></tr><tr><td>eval/lead_adversary/reward/episode_reward_max</td><td></td></tr><tr><td>eval/lead_adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/lead_adversary/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/adversary/alpha</td><td></td></tr><tr><td>train/adversary/entropy</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_actor</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/adversary/loss_actor</td><td></td></tr><tr><td>train/adversary/loss_alpha</td><td></td></tr><tr><td>train/adversary/loss_qvalue</td><td></td></tr><tr><td>train/agent/alpha</td><td></td></tr><tr><td>train/agent/entropy</td><td></td></tr><tr><td>train/agent/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agent/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agent/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agent/loss_actor</td><td></td></tr><tr><td>train/agent/loss_alpha</td><td></td></tr><tr><td>train/agent/loss_qvalue</td><td></td></tr><tr><td>train/lead_adversary/alpha</td><td></td></tr><tr><td>train/lead_adversary/entropy</td><td></td></tr><tr><td>train/lead_adversary/grad_norm_loss_actor</td><td></td></tr><tr><td>train/lead_adversary/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/lead_adversary/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/lead_adversary/loss_actor</td><td></td></tr><tr><td>train/lead_adversary/loss_alpha</td><td></td></tr><tr><td>train/lead_adversary/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/adversary/reward/episode_reward_max</td><td>85</td></tr><tr><td>collection/adversary/reward/episode_reward_mean</td><td>17.66667</td></tr><tr><td>collection/adversary/reward/episode_reward_min</td><td>0</td></tr><tr><td>collection/agent/reward/episode_reward_max</td><td>7.84371</td></tr><tr><td>collection/agent/reward/episode_reward_mean</td><td>-11.45929</td></tr><tr><td>collection/agent/reward/episode_reward_min</td><td>-41.17564</td></tr><tr><td>collection/lead_adversary/reward/episode_reward_max</td><td>85</td></tr><tr><td>collection/lead_adversary/reward/episode_reward_mean</td><td>17.66667</td></tr><tr><td>collection/lead_adversary/reward/episode_reward_min</td><td>0</td></tr><tr><td>collection/reward/episode_reward_max</td><td>42.94145</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>7.95801</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-2.22199</td></tr><tr><td>collection/reward/reward_max</td><td>1.99988</td></tr><tr><td>collection/reward/reward_mean</td><td>-0.11459</td></tr><tr><td>collection/reward/reward_min</td><td>-5.0974</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/adversary/reward/episode_reward_max</td><td>205</td></tr><tr><td>eval/adversary/reward/episode_reward_mean</td><td>28.5</td></tr><tr><td>eval/adversary/reward/episode_reward_min</td><td>0</td></tr><tr><td>eval/agent/reward/episode_reward_max</td><td>23.31415</td></tr><tr><td>eval/agent/reward/episode_reward_mean</td><td>-13.67398</td></tr><tr><td>eval/agent/reward/episode_reward_min</td><td>-109.88651</td></tr><tr><td>eval/lead_adversary/reward/episode_reward_max</td><td>205</td></tr><tr><td>eval/lead_adversary/reward/episode_reward_mean</td><td>28.5</td></tr><tr><td>eval/lead_adversary/reward/episode_reward_min</td><td>0</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>100.03783</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>14.44201</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-2.09331</td></tr><tr><td>timers/collection_time</td><td>13.11212</td></tr><tr><td>timers/evaluation_time</td><td>3.26512</td></tr><tr><td>timers/iteration_time</td><td>59.15569</td></tr><tr><td>timers/total_time</td><td>597.92251</td></tr><tr><td>timers/training_time</td><td>46.04243</td></tr><tr><td>train/adversary/alpha</td><td>0.62193</td></tr><tr><td>train/adversary/entropy</td><td>1.36595</td></tr><tr><td>train/adversary/grad_norm_loss_actor</td><td>0.19935</td></tr><tr><td>train/adversary/grad_norm_loss_alpha</td><td>2.36595</td></tr><tr><td>train/adversary/grad_norm_loss_qvalue</td><td>3.57202</td></tr><tr><td>train/adversary/loss_actor</td><td>-10.00089</td></tr><tr><td>train/adversary/loss_alpha</td><td>-1.12388</td></tr><tr><td>train/adversary/loss_qvalue</td><td>3.42002</td></tr><tr><td>train/agent/alpha</td><td>0.62195</td></tr><tr><td>train/agent/entropy</td><td>1.36546</td></tr><tr><td>train/agent/grad_norm_loss_actor</td><td>0.23034</td></tr><tr><td>train/agent/grad_norm_loss_alpha</td><td>2.36546</td></tr><tr><td>train/agent/grad_norm_loss_qvalue</td><td>1.9755</td></tr><tr><td>train/agent/loss_actor</td><td>-7.23949</td></tr><tr><td>train/agent/loss_alpha</td><td>-1.1236</td></tr><tr><td>train/agent/loss_qvalue</td><td>1.80092</td></tr><tr><td>train/lead_adversary/alpha</td><td>0.62166</td></tr><tr><td>train/lead_adversary/entropy</td><td>1.32524</td></tr><tr><td>train/lead_adversary/grad_norm_loss_actor</td><td>0.80654</td></tr><tr><td>train/lead_adversary/grad_norm_loss_alpha</td><td>2.32524</td></tr><tr><td>train/lead_adversary/grad_norm_loss_qvalue</td><td>4.59415</td></tr><tr><td>train/lead_adversary/loss_actor</td><td>-9.57045</td></tr><tr><td>train/lead_adversary/loss_alpha</td><td>-1.10555</td></tr><tr><td>train/lead_adversary/loss_qvalue</td><td>3.34659</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05/wandb/run-20250119_192605-isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (isac_simple_world_comm_mlp__a15c8d2f_25_01_19-19_26_05) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = 7.95801305770874: 100%|| 10/10 [10:00<00:00, 60.08s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07/wandb/run-20250119_193607-masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07' target=\"_blank\">masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = -520.33154296875: 100%|| 10/10 [02:53<00:00, 17.40s/it] "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/agents/alpha</td><td></td></tr><tr><td>train/agents/entropy</td><td></td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agents/loss_actor</td><td></td></tr><tr><td>train/agents/loss_alpha</td><td></td></tr><tr><td>train/agents/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/reward/episode_reward_max</td><td>-249.0518</td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td>-520.33154</td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td>-958.55646</td></tr><tr><td>collection/reward/episode_reward_max</td><td>-249.0518</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>-520.33154</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-958.55646</td></tr><tr><td>collection/reward/reward_max</td><td>-1.78026</td></tr><tr><td>collection/reward/reward_mean</td><td>-5.20332</td></tr><tr><td>collection/reward/reward_min</td><td>-11.59175</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td>-380.50952</td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td>-524.69513</td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td>-714.72247</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>-380.50952</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>-524.69513</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-714.72247</td></tr><tr><td>timers/collection_time</td><td>2.3577</td></tr><tr><td>timers/evaluation_time</td><td>0.83239</td></tr><tr><td>timers/iteration_time</td><td>17.7093</td></tr><tr><td>timers/total_time</td><td>173.48776</td></tr><tr><td>timers/training_time</td><td>15.35085</td></tr><tr><td>train/agents/alpha</td><td>0.62197</td></tr><tr><td>train/agents/entropy</td><td>1.32414</td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td>0.34977</td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td>2.32414</td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td>58.75512</td></tr><tr><td>train/agents/loss_actor</td><td>48.86283</td></tr><tr><td>train/agents/loss_alpha</td><td>-1.10382</td></tr><tr><td>train/agents/loss_qvalue</td><td>44.02301</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07/wandb/run-20250119_193607-masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (masac_simple_spread_mlp__abd59b28_25_01_19-19_36_07) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = -520.33154296875: 100%|| 10/10 [02:55<00:00, 17.58s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04/wandb/run-20250119_193904-masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04' target=\"_blank\">masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = -41.256187438964844: 100%|| 10/10 [05:01<00:00, 30.19s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/adversary/reward/episode_reward_max</td><td></td></tr><tr><td>collection/adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/adversary/reward/episode_reward_min</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agent/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_max</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/adversary/reward/episode_reward_min</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agent/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/adversary/alpha</td><td></td></tr><tr><td>train/adversary/entropy</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_actor</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/adversary/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/adversary/loss_actor</td><td></td></tr><tr><td>train/adversary/loss_alpha</td><td></td></tr><tr><td>train/adversary/loss_qvalue</td><td></td></tr><tr><td>train/agent/alpha</td><td></td></tr><tr><td>train/agent/entropy</td><td></td></tr><tr><td>train/agent/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agent/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agent/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agent/loss_actor</td><td></td></tr><tr><td>train/agent/loss_alpha</td><td></td></tr><tr><td>train/agent/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/adversary/reward/episode_reward_max</td><td>-15.72659</td></tr><tr><td>collection/adversary/reward/episode_reward_mean</td><td>-102.30374</td></tr><tr><td>collection/adversary/reward/episode_reward_min</td><td>-208.08215</td></tr><tr><td>collection/agent/reward/episode_reward_max</td><td>192.15421</td></tr><tr><td>collection/agent/reward/episode_reward_mean</td><td>19.79135</td></tr><tr><td>collection/agent/reward/episode_reward_min</td><td>-107.86103</td></tr><tr><td>collection/reward/episode_reward_max</td><td>-7.96397</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>-41.25619</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-109.19626</td></tr><tr><td>collection/reward/reward_max</td><td>2.06584</td></tr><tr><td>collection/reward/reward_mean</td><td>0.19791</td></tr><tr><td>collection/reward/reward_min</td><td>-1.33431</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/adversary/reward/episode_reward_max</td><td>-37.92849</td></tr><tr><td>eval/adversary/reward/episode_reward_mean</td><td>-127.16775</td></tr><tr><td>eval/adversary/reward/episode_reward_min</td><td>-220.4415</td></tr><tr><td>eval/agent/reward/episode_reward_max</td><td>94.04778</td></tr><tr><td>eval/agent/reward/episode_reward_mean</td><td>20.95308</td></tr><tr><td>eval/agent/reward/episode_reward_min</td><td>-117.035</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>-10.78256</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>-53.10734</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-77.48174</td></tr><tr><td>timers/collection_time</td><td>2.55152</td></tr><tr><td>timers/evaluation_time</td><td>0.81997</td></tr><tr><td>timers/iteration_time</td><td>30.58148</td></tr><tr><td>timers/total_time</td><td>301.93487</td></tr><tr><td>timers/training_time</td><td>28.02908</td></tr><tr><td>train/adversary/alpha</td><td>0.62197</td></tr><tr><td>train/adversary/entropy</td><td>1.36061</td></tr><tr><td>train/adversary/grad_norm_loss_actor</td><td>0.53962</td></tr><tr><td>train/adversary/grad_norm_loss_alpha</td><td>2.36061</td></tr><tr><td>train/adversary/grad_norm_loss_qvalue</td><td>2.23289</td></tr><tr><td>train/adversary/loss_actor</td><td>1.32576</td></tr><tr><td>train/adversary/loss_alpha</td><td>-1.1212</td></tr><tr><td>train/adversary/loss_qvalue</td><td>0.40312</td></tr><tr><td>train/agent/alpha</td><td>0.62195</td></tr><tr><td>train/agent/entropy</td><td>1.36052</td></tr><tr><td>train/agent/grad_norm_loss_actor</td><td>0.39747</td></tr><tr><td>train/agent/grad_norm_loss_alpha</td><td>2.36052</td></tr><tr><td>train/agent/grad_norm_loss_qvalue</td><td>5.31627</td></tr><tr><td>train/agent/loss_actor</td><td>-10.21705</td></tr><tr><td>train/agent/loss_alpha</td><td>-1.12127</td></tr><tr><td>train/agent/loss_qvalue</td><td>2.241</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04/wandb/run-20250119_193904-masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (masac_simple_adversary_mlp__2b2503cc_25_01_19-19_39_04) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = -41.256187438964844: 100%|| 10/10 [05:04<00:00, 30.47s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/masac_navigation_mlp__348bbcfc_25_01_19-19_44_10/wandb/run-20250119_194410-masac_navigation_mlp__348bbcfc_25_01_19-19_44_10</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_navigation_mlp__348bbcfc_25_01_19-19_44_10' target=\"_blank\">masac_navigation_mlp__348bbcfc_25_01_19-19_44_10</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_navigation_mlp__348bbcfc_25_01_19-19_44_10' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_navigation_mlp__348bbcfc_25_01_19-19_44_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = -0.23322419822216034: 100%|| 10/10 [03:02<00:00, 18.01s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/info/agent_collisions</td><td></td></tr><tr><td>collection/agents/info/final_rew</td><td></td></tr><tr><td>collection/agents/info/pos_rew</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_max</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/episode_reward_max</td><td></td></tr><tr><td>collection/reward/episode_reward_mean</td><td></td></tr><tr><td>collection/reward/episode_reward_min</td><td></td></tr><tr><td>collection/reward/reward_max</td><td></td></tr><tr><td>collection/reward/reward_mean</td><td></td></tr><tr><td>collection/reward/reward_min</td><td></td></tr><tr><td>counters/current_frames</td><td></td></tr><tr><td>counters/iter</td><td></td></tr><tr><td>counters/total_frames</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td></td></tr><tr><td>eval/reward/episode_len_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_max</td><td></td></tr><tr><td>eval/reward/episode_reward_mean</td><td></td></tr><tr><td>eval/reward/episode_reward_min</td><td></td></tr><tr><td>timers/collection_time</td><td></td></tr><tr><td>timers/evaluation_time</td><td></td></tr><tr><td>timers/iteration_time</td><td></td></tr><tr><td>timers/total_time</td><td></td></tr><tr><td>timers/training_time</td><td></td></tr><tr><td>train/agents/alpha</td><td></td></tr><tr><td>train/agents/entropy</td><td></td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td></td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td></td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td></td></tr><tr><td>train/agents/loss_actor</td><td></td></tr><tr><td>train/agents/loss_alpha</td><td></td></tr><tr><td>train/agents/loss_qvalue</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>collection/agents/info/agent_collisions</td><td>-0.002</td></tr><tr><td>collection/agents/info/final_rew</td><td>0</td></tr><tr><td>collection/agents/info/pos_rew</td><td>-0.00033</td></tr><tr><td>collection/agents/reward/episode_reward_max</td><td>0.27195</td></tr><tr><td>collection/agents/reward/episode_reward_mean</td><td>-0.23322</td></tr><tr><td>collection/agents/reward/episode_reward_min</td><td>-4.23443</td></tr><tr><td>collection/reward/episode_reward_max</td><td>0.27195</td></tr><tr><td>collection/reward/episode_reward_mean</td><td>-0.23322</td></tr><tr><td>collection/reward/episode_reward_min</td><td>-4.23443</td></tr><tr><td>collection/reward/reward_max</td><td>0.02614</td></tr><tr><td>collection/reward/reward_mean</td><td>-0.00233</td></tr><tr><td>collection/reward/reward_min</td><td>-1.01708</td></tr><tr><td>counters/current_frames</td><td>6000</td></tr><tr><td>counters/iter</td><td>9</td></tr><tr><td>counters/total_frames</td><td>60000</td></tr><tr><td>eval/agents/reward/episode_reward_max</td><td>0.10935</td></tr><tr><td>eval/agents/reward/episode_reward_mean</td><td>0.00293</td></tr><tr><td>eval/agents/reward/episode_reward_min</td><td>-0.12079</td></tr><tr><td>eval/reward/episode_len_mean</td><td>100</td></tr><tr><td>eval/reward/episode_reward_max</td><td>0.10935</td></tr><tr><td>eval/reward/episode_reward_mean</td><td>0.00293</td></tr><tr><td>eval/reward/episode_reward_min</td><td>-0.12079</td></tr><tr><td>timers/collection_time</td><td>3.1652</td></tr><tr><td>timers/evaluation_time</td><td>1.85818</td></tr><tr><td>timers/iteration_time</td><td>17.96332</td></tr><tr><td>timers/total_time</td><td>182.1282</td></tr><tr><td>timers/training_time</td><td>14.7974</td></tr><tr><td>train/agents/alpha</td><td>0.62196</td></tr><tr><td>train/agents/entropy</td><td>1.36333</td></tr><tr><td>train/agents/grad_norm_loss_actor</td><td>0.1966</td></tr><tr><td>train/agents/grad_norm_loss_alpha</td><td>2.36333</td></tr><tr><td>train/agents/grad_norm_loss_qvalue</td><td>2.46588</td></tr><tr><td>train/agents/loss_actor</td><td>-8.22455</td></tr><tr><td>train/agents/loss_alpha</td><td>-1.12255</td></tr><tr><td>train/agents/loss_qvalue</td><td>1.12792</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">masac_navigation_mlp__348bbcfc_25_01_19-19_44_10</strong> at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_navigation_mlp__348bbcfc_25_01_19-19_44_10' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_navigation_mlp__348bbcfc_25_01_19-19_44_10</a><br> View project at: <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./masac_navigation_mlp__348bbcfc_25_01_19-19_44_10/wandb/run-20250119_194410-masac_navigation_mlp__348bbcfc_25_01_19-19_44_10/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2299: UserWarning: Run (masac_navigation_mlp__348bbcfc_25_01_19-19_44_10) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "\n",
            "mean return = -0.23322419822216034: 100%|| 10/10 [03:04<00:00, 18.49s/it]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py:202: PydanticDeprecatedSince20: The `copy` method is deprecated; use `model_copy` instead. See the docstring of `BaseModel.copy` for details about how to handle `include` and `exclude`. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
            "  settings = self._wl.settings.copy()\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/BenchMARL/masac_simple_world_comm_mlp__d86bbe14_25_01_19-19_47_16/wandb/run-20250119_194716-masac_simple_world_comm_mlp__d86bbe14_25_01_19-19_47_16</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_world_comm_mlp__d86bbe14_25_01_19-19_47_16' target=\"_blank\">masac_simple_world_comm_mlp__d86bbe14_25_01_19-19_47_16</a></strong> to <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_world_comm_mlp__d86bbe14_25_01_19-19_47_16' target=\"_blank\">https://wandb.ai/01019799-marcingodniak/benchmarl/runs/masac_simple_world_comm_mlp__d86bbe14_25_01_19-19_47_16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "mean return = 10.188268661499023:  10%|         | 1/10 [01:01<09:12, 61.41s/it]"
          ]
        }
      ],
      "source": [
        "# Run benchmark and get JSON files\n",
        "experiments_json_files = run_benchmark()\n",
        "\n",
        "# Process results\n",
        "processed_data, environment_comparison_matrix, sample_efficiency_matrix = process_results(experiments_json_files)\n",
        "\n",
        "# Generate plots\n",
        "generate_plots(\n",
        "    processed_data, environment_comparison_matrix, sample_efficiency_matrix\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnBVPfNJWAjm"
      },
      "source": [
        "## Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYk4pvbzWAjm"
      },
      "source": [
        "### Analysis reproduction\n",
        "\n",
        "The analysys was computed using Google Colab environment within the notebook. The wandb account is required for logging purposes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLPY9aaRWAjn"
      },
      "source": [
        "### Pre-result discussion\n",
        "\n",
        "The original conception has leaned towards the tsask from a MeltingPot environment. However, during testing process, the CNN networks representing Actors and Critics necessary to process visual environment provided by a task has proven to be too costly in computation power to acquire substantive results. Therfore, the decision has been made to pivot towards [VMAS](https://github.com/proroklab/VectorizedMultiAgentSimulator) environment with more simple tasks that support model architecture on level of MLP. Tasks and upcoming performance of the algorithms has been reported below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-8b2dq8WAjn"
      },
      "source": [
        "### Task: Simple adversary (Adversary)\n",
        "#### Description\n",
        "1 adversary (red), N good agents (green), N landmarks (usually N=2). All agents observe position of landmarks and other agents. One landmark is the target landmark (colored green). Good agents rewarded based on how close one of them is to the target landmark, but negatively rewarded if the adversary is close to target landmark. Adversary is rewarded based on how close it is to the target, but it doesnt know which landmark is the target landmark. So good agents have to learn to split up and cover all landmarks to deceive the adversary.\n",
        "\n",
        "#### Parametrization\n",
        "The task has been parametrized using hydra as:\n",
        "\n",
        "```yaml\n",
        "max_steps: 100\n",
        "n_agents: 3\n",
        "n_adversaries: 1\n",
        "```\n",
        "\n",
        "#### ISAC vs MASAC\n",
        "##### Training\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/adversary-train1.png?raw=1)\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/adversary-train2.png?raw=1)\n",
        "\n",
        "There are two key aspects we should pay attention during training: (1) Coordination, (2) Exploration. Our expectations evolve around ISAC with independent critics having more troubles to learn to coordinate agents efforts, but discovering more varied policies. On the other hand the MASAC with critic covering global interactions should have less trouble with coordination while performing faster convergence.\n",
        "\n",
        "Based on a logs above we can observe that the *train/agent/entropy* does support the claim of ISAC's higher policy entropy, however the convergence speed as shown on agent and adversary loss Q-values do not make it look like a MSAC is converging significantly faster, quite the opposit - MSAC show tendences to initially plateu on a worse values then ISAC and only in remaining time slowly converge to around the end outperform individual critics. This might be a property of a costly general critic training, that however if trained beter generalises the environment then an agent-specified ones. What should be pointed out is that although the Q-value does not converge, the entropy influence parameter alpha does indeed in case of MSAC fall faster along with the agent entropy.\n",
        "\n",
        "##### Evaluation\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/aversary-eval.png?raw=1)\n",
        "\n",
        "During evaluation one can observe the ISAC algorithm performing with higher variety in minimal and maximal reward of used policies due to the higher entropy of individual learning with low degree of effort coordination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwb3W4mjWAjo"
      },
      "source": [
        "### Task: Simple Spread (Cooperation)\n",
        "#### Description\n",
        "N agents, N landmarks. Agents are rewarded based on how far any agent is from each landmark. Agents are penalized if they collide with other agents. So, agents have to learn to cover all the landmarks while avoiding collisions.\n",
        "\n",
        "#### Parametrization\n",
        "The task has been parametrized using hydra as:\n",
        "```yaml\n",
        "max_steps: 100\n",
        "n_agents: 3\n",
        "obs_agents: True\n",
        "```\n",
        "The agent observability has been added to make learning the patterns easier using the other agents positions in a frame of reference.\n",
        "\n",
        "#### ISAC vs MASAC\n",
        "##### Training\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/spread-train.png?raw=1)\n",
        "\n",
        "In this situation, the expected consequence of a global vs independend critic struggle is collision learning: the MASAC is set up to jointly learn collision avoidance while ISAC will have to independently learn paths of the others. The normalized loss gradients of both critic and actor in MASAC show signinficantly higher values, presumably due to high awarness and therefore incorporation of the collision penalty. One can also observe the expected Q-value (critic) is more reliable within centralized critic (MASAC), probably because of the collision loss effective modeling. Entropy supports the claim with ISAC behaving with higher variance without a centralized policy verification.\n",
        "\n",
        "##### Evaluation\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/spread-eval.png?raw=1)\n",
        "\n",
        "As shown during evaluation, the MASAC has proven to take sizable lead in learning the cooperative task. After the 250 steps, it has reliably been delivering better mean results then ISAC, have also mostly been outperforming the other algorithm within minimum reward (except some hicup on step 400 evaluation - probably a collision penalty). Interesting part that could be observed is relative stability of the maximal rewards earned by the algorithms, as they do not incorporate mostly the very swingy collision penalties: worth pointing out is a relative competitivness of both algorithms on steps between 150 and 300. This moment in time could probably be the one, where global MASAC critic hasn't learned to avoid collisions reliably yet while ISAC independent critics were able to optimize the attraction fields coverage navigation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oM8kgTK6WAjo"
      },
      "source": [
        "### Task: Navigation (very light coop)\n",
        "\n",
        "#### Description\n",
        "Randomly spawned agents need to navigate to their goal. Collisions can be turned on and agents can use LIDARs to avoid running into each other. Rewards were turned to be shared in form of L2 distance. Apart from position, velocity, and lidar readings, each agent can be set up to observe just the relative distance to its goal, or its relative distance to all goals (in this case the task needs heterogeneous behavior to be solved).\n",
        "\n",
        "#### Parametrization\n",
        "The task has been parametrized using hydra as:\n",
        "```yaml\n",
        "max_steps: 100\n",
        "n_agents: 3\n",
        "collisions: True\n",
        "agents_with_same_goal: 1\n",
        "split_goals: False\n",
        "observe_all_goals: False\n",
        "shared_rew: False\n",
        "lidar_range: 0.35\n",
        "agent_radius: 0.1\n",
        "```\n",
        "The task is cooperative in terms of collisions, as agents are trying to avoid those. However as the rewards are not shared and the goals are not split, it is a very light interaction as for a multi-agent environment.\n",
        "\n",
        "#### ISAC vs MASAC\n",
        "##### Training\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/navigation-train.png?raw=1)\n",
        "\n",
        "Due to incorporation of collision penalties in global MASAC critic, the agents of this algorithm show slower convergence and high entropy. What is interesting, the high entropy was expected to be shown during ISAC analysis, but apparently the dirastic changes to a learning central critic has forced individual actors to adapt on the fly. In stage between 100-150 steps the interesting behaviour is exhibited by an entropy where agents entropy has polarized from around 1 to -1 as their policies during training emerged. ISAC was the first algorithm to start optimizing around entropy, while MASAC was still presumably trying to model the collision influence. Overall, the training concluded with MASAC having marginally smaller losses on its actors.\n",
        "\n",
        "\n",
        "##### Evaluation\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/navigation-eval.png?raw=1)\n",
        "\n",
        "The evaluation has clearly identified the objectives different algorithms pursuit. By the episode length mean, we can learn it was in fact ISAC that learned to actually FINISH the task it was given - transport all the agents to their attraction fields, while MASAC was never able to compleate the task and was cut off on the episode 100. However, all the other statistics show clear supriority of the MASAC outcomes, even if it was not able to reach objective, the ISAC means were imposing too much losses. By the step 100, the MSAC confidently outperformed ISAC in reward collection in all fields: min, mean and max. Thus it could have been learned that paramisation of the environment had clear restrictions on collisions, to which central critic was able to adapt much better then the individuals."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohlPCP--WAjp"
      },
      "source": [
        "### Task: Simple World Comm (Mixed)\n",
        "\n",
        "#### Description\n",
        "Predator-prey environment. Good agents (green) are faster and want to avoid being hit by adversaries (red). Adversaries are slower and want to hit good agents. Obstacles (large black circles) block the way. Moreover, (1) there is food (small blue balls) that the good agents are rewarded for being near, (2) we now have forests that hide agents inside from being seen from outside; (3) there is a leader adversary that can see the agents at all times, and can communicate with the other adversaries to help coordinate the chase.\n",
        "\n",
        "#### Parametrization\n",
        "The task has been parametrized using hydra as:\n",
        "```yaml\n",
        "max_steps: 100\n",
        "num_good_agents: 2\n",
        "num_adversaries: 4\n",
        "num_landmarks: 1\n",
        "num_food: 2\n",
        "num_forests: 2\n",
        "```\n",
        "\n",
        "#### ISAC vs MASAC\n",
        "##### Training\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/world-train1.png?raw=1)\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/world-train2.png?raw=1)\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/world-train3.png?raw=1)\n",
        "As shown on above panels, both algorithms did have same learning rates mechanisms as gradients and alpha seems to remain same. [TODO]()\n",
        "##### Evaluation\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/world-eval.png?raw=1)\n",
        "![](https://github.com/RichardStaszkiewicz/WUT-USD-MASCvsISAC/blob/main/graphics/world-eval2.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBrZCfKJWAjp"
      },
      "source": [
        "### Discussion\n",
        "\n",
        "The whole research was mostly boiled down to a **centralized (MASAC) vs. independedt (ISAC) critic**. The centralized version allows the critic to better model the environment's dynamics and the interaction between agents and adversaries, however is more costly in computation.\n",
        "\n",
        "\n",
        "\n",
        "the final comparison table"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}